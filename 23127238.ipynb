{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOD5baC9Pz39"
      },
      "outputs": [],
      "source": [
        "!pip install arxiv requests psutil memory_profiler\n",
        "!apt-get install -y file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile arxiv_crawler.py\n",
        "import arxiv\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import tarfile\n",
        "import shutil\n",
        "import subprocess\n",
        "import gzip\n",
        "\n",
        "SAVE_DIR = \"./23127238\"\n",
        "\n",
        "def detect_and_fix_filetype(tar_path):\n",
        "    try:\n",
        "        result = subprocess.run([\"file\", tar_path], capture_output=True, text=True, errors='ignore')\n",
        "        output = result.stdout.strip()\n",
        "    except FileNotFoundError:\n",
        "        print(\"X 'file' command not found. Install 'file' utility.\")\n",
        "        return tar_path, \"unknown\", None\n",
        "    except Exception as e:\n",
        "        print(f\"X Error running 'file': {e}\")\n",
        "        return tar_path, \"unknown\", None\n",
        "\n",
        "    if \"PDF document\" in output:\n",
        "        print(f\"  -> Detected PDF: {os.path.basename(tar_path)}\")\n",
        "        return tar_path, \"pdf\", None\n",
        "    elif \"gzip compressed data\" in output:\n",
        "        match = re.search(r', was \"([^\"]+)\"', output)\n",
        "        if match:\n",
        "            return tar_path, \"gz\", os.path.basename(match.group(1))\n",
        "        else:\n",
        "            return tar_path, \"tar.gz\", None\n",
        "    elif \"tar archive\" in output:\n",
        "        return tar_path, \"tar.gz\", None\n",
        "    else:\n",
        "        print(f\"  Unknown format: {output}\")\n",
        "        return tar_path, \"unknown\", None\n",
        "\n",
        "\n",
        "def extract_and_clean(tar_path, dest_folder, base_name):\n",
        "    fixed_path, filetype, orig_name = detect_and_fix_filetype(tar_path)\n",
        "    extract_path = os.path.join(dest_folder, base_name)\n",
        "    os.makedirs(extract_path, exist_ok=True)\n",
        "    deleted = 0\n",
        "\n",
        "    if filetype == \"pdf\":\n",
        "        return (os.path.basename(tar_path), True, 0, \"pdf\")\n",
        "    if filetype == \"unknown\":\n",
        "        return (os.path.basename(tar_path), False, 0, \"unknown\")\n",
        "\n",
        "    try:\n",
        "        if filetype == \"tar.gz\":\n",
        "            with tarfile.open(fixed_path, 'r:*') as tar:\n",
        "                tar.extractall(path=extract_path)\n",
        "        elif filetype == \"gz\":\n",
        "            out_name = orig_name or f\"{base_name}.file\"\n",
        "            out_path = os.path.join(extract_path, out_name)\n",
        "            with gzip.open(fixed_path, 'rb') as fin, open(out_path, 'wb') as fout:\n",
        "                shutil.copyfileobj(fin, fout)\n",
        "    except Exception as e:\n",
        "        print(f\"X Extract error: {e}\")\n",
        "        shutil.rmtree(extract_path, ignore_errors=True)\n",
        "        return (os.path.basename(tar_path), False, 0, \"extract_fail\")\n",
        "\n",
        "    # Clean: keep only .tex and .bib\n",
        "    for root, _, files in os.walk(extract_path):\n",
        "        for f in files:\n",
        "            if not f.lower().endswith(('.tex', '.bib')):\n",
        "                try:\n",
        "                    os.remove(os.path.join(root, f))\n",
        "                    deleted += 1\n",
        "                except:\n",
        "                    pass\n",
        "    return (os.path.basename(tar_path), True, deleted, \"ok\")\n",
        "\n",
        "\n",
        "def crawl_single_paper(arxiv_id, save_dir=SAVE_DIR):\n",
        "    \"\"\"\n",
        "    Download and process a single arXiv paper with all its versions.\n",
        "\n",
        "    Args:\n",
        "        arxiv_id: arXiv ID in format yymm.nnnnn (e.g., \"2305.04793\")\n",
        "        save_dir: Directory to save the paper data\n",
        "\n",
        "    Returns:\n",
        "        bool: True if successful, False otherwise\n",
        "    \"\"\"\n",
        "    client = arxiv.Client()\n",
        "    paper_folder = None\n",
        "    tex_folder = None\n",
        "    versions_processed = 0\n",
        "    latest_version = 0\n",
        "\n",
        "    # Validate and split ID\n",
        "    if '.' not in arxiv_id:\n",
        "        print(f\"X Invalid arxiv_id: {arxiv_id}\")\n",
        "        return False\n",
        "\n",
        "    prefix, suffix = arxiv_id.split('.')\n",
        "    paper_folder = os.path.join(save_dir, f\"{prefix}-{suffix}\")\n",
        "    tex_folder = os.path.join(paper_folder, \"tex\")\n",
        "    os.makedirs(tex_folder, exist_ok=True)\n",
        "\n",
        "    # Get latest version from v1\n",
        "    try:\n",
        "        search = arxiv.Search(id_list=[arxiv_id])\n",
        "        base_paper = next(client.results(search))\n",
        "        match = re.search(r'v(\\d+)$', base_paper.entry_id)\n",
        "        latest_version = int(match.group(1)) if match else 1\n",
        "        print(f\"[{arxiv_id}] Found {latest_version} version(s)\")\n",
        "    except StopIteration:\n",
        "        print(f\"X [{arxiv_id}] Paper not found\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"X [{arxiv_id}] Error finding latest version: {e}\")\n",
        "        return False\n",
        "\n",
        "    # Collect metadata from v1\n",
        "    title = base_paper.title\n",
        "    authors = [a.name for a in base_paper.authors]\n",
        "    submission_date = base_paper.published.strftime(\"%Y-%m-%d\") if base_paper.published else None\n",
        "    publication_venue = base_paper.journal_ref if base_paper.journal_ref else None\n",
        "    categories = base_paper.categories\n",
        "    abstract = base_paper.summary.replace(\"\\n\", \" \").strip()\n",
        "    pdf_url = base_paper.pdf_url\n",
        "    revised_dates = []\n",
        "\n",
        "    # Get revised dates for v2..vN\n",
        "    if latest_version > 1:\n",
        "        for v in range(2, latest_version + 1):\n",
        "            try:\n",
        "                vid = f\"{arxiv_id}v{v}\"\n",
        "                search_v = arxiv.Search(id_list=[vid])\n",
        "                paper_v = next(client.results(search_v))\n",
        "                revised_dates.append(paper_v.updated.strftime(\"%Y-%m-%d\") if paper_v.updated else None)\n",
        "            except:\n",
        "                revised_dates.append(None)\n",
        "\n",
        "    # Save metadata.json\n",
        "    metadata = {\n",
        "        \"arxiv_id\": arxiv_id.replace('.', '-'),\n",
        "        \"paper_title\": title,\n",
        "        \"authors\": authors,\n",
        "        \"submission_date\": submission_date,\n",
        "        \"revised_dates\": revised_dates,\n",
        "        \"publication_venue\": publication_venue,\n",
        "        \"latest_version\": latest_version,\n",
        "        \"categories\": categories,\n",
        "        \"abstract\": abstract,\n",
        "        \"pdf_url\": pdf_url,\n",
        "    }\n",
        "\n",
        "    metadata_path = os.path.join(paper_folder, \"metadata.json\")\n",
        "    try:\n",
        "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(metadata, f, indent=4, ensure_ascii=False)\n",
        "        print(f\"  [{arxiv_id}] Saved metadata.json\")\n",
        "    except Exception as e:\n",
        "        print(f\"X [{arxiv_id}] Failed to save metadata: {e}\")\n",
        "        return False\n",
        "\n",
        "    # Download all versions into tex folder\n",
        "    for v in range(1, latest_version + 1):\n",
        "        version_id = f\"{arxiv_id}v{v}\"\n",
        "        version_folder_name = f\"{prefix}-{suffix}v{v}\"\n",
        "        temp_tar = os.path.join(paper_folder, f\"{version_id}.tar.gz\")\n",
        "\n",
        "        try:\n",
        "            search_v = arxiv.Search(id_list=[version_id])\n",
        "            paper_v = next(client.results(search_v))\n",
        "\n",
        "            print(f\"  [{arxiv_id}] Downloading {version_id}...\")\n",
        "            paper_v.download_source(dirpath=paper_folder, filename=f\"{version_id}.tar.gz\")\n",
        "\n",
        "            # Extract & Clean into tex folder\n",
        "            file_name, success, deleted_count, ftype = extract_and_clean(temp_tar, tex_folder, version_folder_name)\n",
        "\n",
        "            if success:\n",
        "                versions_processed += 1\n",
        "                print(f\"  [{arxiv_id}] Extracted & cleaned: {version_id} ({deleted_count} files removed)\")\n",
        "            else:\n",
        "                print(f\"X [{arxiv_id}] Failed to extract {version_id}\")\n",
        "\n",
        "            # Delete .tar.gz\n",
        "            try:\n",
        "                os.remove(temp_tar)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            time.sleep(0.5)  # Be nice to arXiv\n",
        "\n",
        "        except StopIteration:\n",
        "            print(f\"X [{arxiv_id}] Version {version_id} not found\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"X [{arxiv_id}] Download error {version_id}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Final check\n",
        "    success = (versions_processed > 0)\n",
        "    if success:\n",
        "        print(f\"✓ [{arxiv_id}] COMPLETED ({versions_processed}/{latest_version} versions)\")\n",
        "    else:\n",
        "        print(f\"X [{arxiv_id}] FAILED - no versions downloaded\")\n",
        "\n",
        "    return success"
      ],
      "metadata": {
        "id": "u0l5xvk3P7el"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reference_extractor.py\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "\n",
        "\n",
        "def format_arxiv_id_for_key(arxiv_id):\n",
        "    \"\"\"\n",
        "    Convert arXiv ID to folder format (yymm-nnnnn).\n",
        "    Examples:\n",
        "        \"2305.04793\" -> \"2305-04793\"\n",
        "        \"2305.04793v1\" -> \"2305-04793\"\n",
        "    \"\"\"\n",
        "    # Remove version suffix if present\n",
        "    clean_id = re.sub(r'v\\d+$', '', arxiv_id)\n",
        "    # Replace dot with dash\n",
        "    return clean_id.replace('.', '-')\n",
        "\n",
        "\n",
        "def get_paper_references(arxiv_id, delay=3):\n",
        "    \"\"\"\n",
        "    Fetch references for a paper from Semantic Scholar API.\n",
        "    Retries indefinitely until success or 404.\n",
        "\n",
        "    Args:\n",
        "        arxiv_id: arXiv ID (format: YYMM.NNNNN or YYMM.NNNNNvN)\n",
        "        delay: delay between retries in seconds\n",
        "\n",
        "    Returns:\n",
        "        tuple: (list of references, total_found_count) or (None, 0) if 404 error\n",
        "    \"\"\"\n",
        "    # Clean arxiv_id (remove version suffix if present)\n",
        "    clean_id = re.sub(r'v\\d+$', '', arxiv_id)\n",
        "    url = f\"https://api.semanticscholar.org/graph/v1/paper/arXiv:{clean_id}\"\n",
        "    params = {\n",
        "        \"fields\": \"references,references.title,references.authors,references.year,references.venue,references.externalIds,references.publicationDate\"\n",
        "    }\n",
        "\n",
        "    ### My SEMANTIC_SCHOLAR_API_KEY\n",
        "    API_KEY = os.getenv(\"a8okwqTLp18Ku1vBXJ1Jb6eRoDKpmAem41VjtFCY\")\n",
        "    headers = {}\n",
        "    if API_KEY:\n",
        "        headers[\"x-api-key\"] = API_KEY\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            #response = requests.get(url, params=params, timeout=10)\n",
        "            response = requests.get(url, params=params, headers=headers, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                references = data.get(\"references\", [])\n",
        "                total_found = len(references) if references else 0\n",
        "                return references, total_found\n",
        "            elif response.status_code == 429:\n",
        "                print(f\"  [{arxiv_id}] Rate limit hit. Waiting {delay}s...\")\n",
        "                time.sleep(delay)\n",
        "            elif response.status_code == 404:\n",
        "                print(f\"  [{arxiv_id}] Paper not found in Semantic Scholar (404)\")\n",
        "                return None, 0  # Return None to indicate 404 error\n",
        "            else:\n",
        "                print(f\"  [{arxiv_id}] API returned status {response.status_code}, retrying in {delay}s...\")\n",
        "                time.sleep(delay)\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"  [{arxiv_id}] Request error: {e}, retrying in {delay}s...\")\n",
        "            time.sleep(delay)\n",
        "\n",
        "\n",
        "def convert_to_references_dict(references):\n",
        "    \"\"\"\n",
        "    Convert Semantic Scholar references to the required format:\n",
        "    Dictionary with arXiv IDs as keys (in \"yyyymm-id\" format) for papers with arXiv IDs.\n",
        "\n",
        "    Args:\n",
        "        references: List of references from Semantic Scholar API\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with paper IDs as keys and metadata as values\n",
        "    \"\"\"\n",
        "    result = {}\n",
        "\n",
        "    for ref in references:\n",
        "        # Skip if reference is None or empty\n",
        "        if not ref:\n",
        "            continue\n",
        "\n",
        "        # Extract external IDs (may be None)\n",
        "        external_ids = ref.get(\"externalIds\", {})\n",
        "        if external_ids is None:\n",
        "            external_ids = {}\n",
        "\n",
        "        arxiv_id = external_ids.get(\"ArXiv\", \"\")\n",
        "\n",
        "        # Only keep references that have arXiv_id\n",
        "        if not arxiv_id:\n",
        "            continue\n",
        "\n",
        "        # Use arXiv ID in yyyymm-id format\n",
        "        key = format_arxiv_id_for_key(arxiv_id)\n",
        "\n",
        "        # Extract authors\n",
        "        authors_list = ref.get(\"authors\", [])\n",
        "        authors = [author.get(\"name\", \"\") for author in authors_list if author.get(\"name\")]\n",
        "\n",
        "        # Extract dates (use publicationDate if available)\n",
        "        publication_date = ref.get(\"publicationDate\", \"\")\n",
        "        year = ref.get(\"year\")\n",
        "\n",
        "        # If no publication date but have year, create an ISO-like format\n",
        "        if not publication_date and year:\n",
        "            publication_date = f\"{year}-01-01\"  # Use Jan 1st as placeholder\n",
        "\n",
        "        # Build metadata dictionary with required fields\n",
        "        metadata = {\n",
        "            \"paper_title\": ref.get(\"title\", \"\"),\n",
        "            \"authors\": authors,\n",
        "            \"submission_date\": publication_date if publication_date else \"\",\n",
        "            \"semantic_scholar_id\": ref.get(\"paperId\"),\n",
        "            \"year\": year\n",
        "        }\n",
        "\n",
        "        result[key] = metadata\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def extract_references_for_paper(arxiv_id, save_dir=\"./23127238\"):\n",
        "    \"\"\"\n",
        "    Extract references for a paper and save to references.json.\n",
        "\n",
        "    Args:\n",
        "        arxiv_id: arXiv ID in format yymm.nnnnn (e.g., \"2305.04793\")\n",
        "        save_dir: Base directory containing paper folders\n",
        "\n",
        "    Returns:\n",
        "        bool: True if successful (found and saved references), False otherwise\n",
        "    \"\"\"\n",
        "    # Convert arxiv_id to folder format\n",
        "    paper_id_key = format_arxiv_id_for_key(arxiv_id)\n",
        "    paper_folder = os.path.join(save_dir, paper_id_key)\n",
        "\n",
        "    # Check if the folder exists\n",
        "    if not os.path.exists(paper_folder):\n",
        "        print(f\"X [{arxiv_id}] Paper folder not found: {paper_folder}\")\n",
        "        return False\n",
        "\n",
        "    print(f\"[{arxiv_id}] Fetching references...\")\n",
        "\n",
        "    try:\n",
        "        json_path = os.path.join(paper_folder, \"references.json\")\n",
        "        references, total_found = get_paper_references(arxiv_id)\n",
        "\n",
        "        # If we got None (404 error), save empty file and return failure\n",
        "        if references is None:\n",
        "            print(f\"X [{arxiv_id}] Failed to fetch references from Semantic Scholar (404)\")\n",
        "            with open(json_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump({}, f, indent=4, ensure_ascii=False)\n",
        "            return False\n",
        "\n",
        "        if not references or total_found == 0:\n",
        "            print(f\"X [{arxiv_id}] No references found (total_found: 0)\")\n",
        "            # Save empty dict but return False\n",
        "            with open(json_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump({}, f, indent=4, ensure_ascii=False)\n",
        "            return False\n",
        "\n",
        "        references_dict = convert_to_references_dict(references)\n",
        "        total_saved = len(references_dict)\n",
        "\n",
        "        # Save only the references dict (no statistics in JSON)\n",
        "        with open(json_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(references_dict, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "        # Log statistics to console only\n",
        "        print(f\"✓ [{arxiv_id}] Found {total_found} references, saved {total_saved} (with arXiv IDs) to references.json\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"X [{arxiv_id}] Error extracting references: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "QApGhns2P8U8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import psutil\n",
        "import threading\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from threading import Lock\n",
        "from arxiv_crawler import crawl_single_paper\n",
        "from reference_extractor import extract_references_for_paper\n",
        "\n",
        "\n",
        "# Global statistics\n",
        "stats_lock = Lock()\n",
        "stats = {\n",
        "    \"total_processed\": 0,           # Total number of papers processed\n",
        "    \"both_success\": 0,              # Both 2 parts are successful\n",
        "    \"only_crawler_success\": 0,      # Only successful crawlers\n",
        "    \"only_references_success\": 0,   # Only successful references (not logical)\n",
        "    \"crawler_failed\": 0,            # Crawler Failed\n",
        "    \"references_failed\": 0,         # References failed (crawler successful)\n",
        "    \"both_failed\": 0,               # Both 2 parts failed\n",
        "}\n",
        "\n",
        "monitor_running = True\n",
        "ram_samples_bytes = []\n",
        "peak_disk_usage_bytes = 0\n",
        "\n",
        "def _monitor_resources(baseline_ram, baseline_disk, sleep_interval=2):\n",
        "    \"\"\"\n",
        "    Runs in the background to monitor average RAM and peak Disk.\n",
        "    \"\"\"\n",
        "    global ram_samples_bytes, peak_disk_usage_bytes, monitor_running\n",
        "\n",
        "    # Reset\n",
        "    ram_samples_bytes = []\n",
        "    peak_disk_usage_bytes = 0\n",
        "\n",
        "    print(f\"[Monitor] Start (Baseline RAM: {baseline_ram / (1024**3):.2f} GB, Baseline Disk: {baseline_disk / (1024**3):.2f} GB)\")\n",
        "\n",
        "    while monitor_running:\n",
        "        try:\n",
        "            # 1. Đo RAM\n",
        "            current_ram = psutil.virtual_memory().used\n",
        "            ram_above_baseline = current_ram - baseline_ram\n",
        "            ram_samples_bytes.append(ram_above_baseline)\n",
        "\n",
        "            # 2. Đo Disk\n",
        "            current_disk = shutil.disk_usage('/').used\n",
        "            disk_above_baseline = current_disk - baseline_disk\n",
        "\n",
        "            if disk_above_baseline > peak_disk_usage_bytes:\n",
        "                peak_disk_usage_bytes = disk_above_baseline\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[Monitor Error] {e}\")\n",
        "\n",
        "        time.sleep(sleep_interval)\n",
        "\n",
        "    print(\"[Monitor] Stop.\")\n",
        "\n",
        "def _print_custom_resource_report(disk_start, disk_end):\n",
        "    \"\"\"\n",
        "    Print reports for missing requests\n",
        "    \"\"\"\n",
        "    global ram_samples_bytes, peak_disk_usage_bytes\n",
        "\n",
        "    #1. Average RAM\n",
        "    avg_ram_bytes = 0\n",
        "    if ram_samples_bytes:\n",
        "        avg_ram_bytes = sum(ram_samples_bytes) / len(ram_samples_bytes)\n",
        "\n",
        "    # 2. The Last Disk\n",
        "    final_disk_bytes = disk_end - disk_start\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ADDITIONAL RESOURCE REPORT (MINUS BASELINE)\")\n",
        "\n",
        "    print(f\"  Average RAM : {avg_ram_bytes / (1024**2):.2f} MB\")\n",
        "    print(f\"  Peak Disk   : {peak_disk_usage_bytes / (1024**2):.2f} MB\")\n",
        "    print(f\"  Final Disk  : {final_disk_bytes / (1024**2):.2f} MB\")\n",
        "\n",
        "    print(\"=\"*80)\n",
        "\n",
        "def process_paper(arxiv_id, save_dir=\"./23127238\"):\n",
        "    \"\"\"\n",
        "    Process a single paper: crawl data first, then extract references.\n",
        "\n",
        "    Args:\n",
        "        arxiv_id: arXiv ID in format yymm.nnnnn\n",
        "        save_dir: Directory to save data\n",
        "\n",
        "    Returns:\n",
        "        tuple: (arxiv_id, crawler_success, references_success)\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Processing paper: {arxiv_id}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Step 1: Crawl paper data\n",
        "    crawler_success = crawl_single_paper(arxiv_id, save_dir)\n",
        "\n",
        "    # Step 2: Extract references (only if crawler succeeded)\n",
        "    references_success = False\n",
        "    if crawler_success:\n",
        "        references_success = extract_references_for_paper(arxiv_id, save_dir)\n",
        "    else:\n",
        "        print(f\"X [{arxiv_id}] Skipping reference extraction (crawler failed)\")\n",
        "\n",
        "    # Update statistics\n",
        "    with stats_lock:\n",
        "        stats[\"total_processed\"] += 1\n",
        "\n",
        "        if crawler_success and references_success:\n",
        "            stats[\"both_success\"] += 1\n",
        "        elif crawler_success and not references_success:\n",
        "            stats[\"only_crawler_success\"] += 1\n",
        "            stats[\"references_failed\"] += 1\n",
        "        elif not crawler_success:\n",
        "            stats[\"crawler_failed\"] += 1\n",
        "            if references_success:\n",
        "                stats[\"only_references_success\"] += 1\n",
        "            else:\n",
        "                stats[\"both_failed\"] += 1\n",
        "\n",
        "    return arxiv_id, crawler_success, references_success\n",
        "\n",
        "\n",
        "def check_paper_exists(arxiv_id, save_dir=\"./23127238\"):\n",
        "    \"\"\"\n",
        "    Check if a paper exists by attempting to crawl it.\n",
        "\n",
        "    Args:\n",
        "        arxiv_id: arXiv ID in format yymm.nnnnn\n",
        "        save_dir: Directory to save data\n",
        "\n",
        "    Returns:\n",
        "        bool: True if paper exists, False otherwise\n",
        "    \"\"\"\n",
        "    success = crawl_single_paper(arxiv_id, save_dir)\n",
        "\n",
        "    # If failed, clean up any created folders\n",
        "    if not success:\n",
        "        prefix, suffix = arxiv_id.split('.')\n",
        "        paper_folder = os.path.join(save_dir, f\"{prefix}-{suffix}\")\n",
        "        if os.path.exists(paper_folder):\n",
        "            try:\n",
        "                shutil.rmtree(paper_folder)\n",
        "                print(f\"  Cleaned up folder for non-existent paper: {arxiv_id}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Warning: Could not clean up folder: {e}\")\n",
        "\n",
        "    return success\n",
        "\n",
        "\n",
        "def find_last_valid_id(prefix, start_id, save_dir=\"./23127238\"):\n",
        "    \"\"\"\n",
        "    Find the last valid paper ID in a month by checking consecutive failures.\n",
        "\n",
        "    Args:\n",
        "        prefix: Month prefix (e.g., \"2305\")\n",
        "        start_id: Starting ID to check from\n",
        "        save_dir: Directory to save data\n",
        "\n",
        "    Returns:\n",
        "        int: Last valid ID found, or 0 if none found\n",
        "    \"\"\"\n",
        "    consecutive_failures = 0\n",
        "    max_consecutive_failures = 3\n",
        "    current_id = start_id\n",
        "    last_valid_id = start_id - 1\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Finding last valid ID for {prefix}.xxxxx starting from {start_id}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    while consecutive_failures < max_consecutive_failures:\n",
        "        arxiv_id = f\"{prefix}.{current_id:05d}\"\n",
        "        print(f\"\\nProbing: {arxiv_id}\")\n",
        "\n",
        "        exists = check_paper_exists(arxiv_id, save_dir)\n",
        "\n",
        "        if exists:\n",
        "            consecutive_failures = 0\n",
        "            last_valid_id = current_id\n",
        "            print(f\"✓ Found valid paper: {arxiv_id}\")\n",
        "        else:\n",
        "            consecutive_failures += 1\n",
        "            print(f\"X Paper not found: {arxiv_id} (failure {consecutive_failures}/{max_consecutive_failures})\")\n",
        "\n",
        "        current_id += 1\n",
        "        time.sleep(0.5)  # Be nice to arXiv\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Last valid ID found: {prefix}.{last_valid_id:05d}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    return last_valid_id\n",
        "\n",
        "\n",
        "def generate_paper_ids(start_month, start_id, end_month, end_id, save_dir=\"./23127238\"):\n",
        "    \"\"\"\n",
        "    Generate list of arXiv IDs based on date range.\n",
        "\n",
        "    Args:\n",
        "        start_month: Start month in format \"YYYY-MM\"\n",
        "        start_id: Starting ID number\n",
        "        end_month: End month in format \"YYYY-MM\"\n",
        "        end_id: Ending ID number\n",
        "        save_dir: Directory to save data\n",
        "\n",
        "    Returns:\n",
        "        list: List of arXiv IDs in format \"yymm.nnnnn\"\n",
        "    \"\"\"\n",
        "    start_year, start_mon = start_month.split('-')\n",
        "    end_year, end_mon = end_month.split('-')\n",
        "    start_prefix = start_year[2:] + start_mon\n",
        "    end_prefix = end_year[2:] + end_mon\n",
        "\n",
        "    paper_ids = []\n",
        "\n",
        "    if start_month == end_month:\n",
        "        # Same month - simple range\n",
        "        print(f\"Single month mode: {start_prefix}.{start_id:05d} → {start_prefix}.{end_id:05d}\")\n",
        "        for i in range(start_id, end_id + 1):\n",
        "            paper_ids.append(f\"{start_prefix}.{i:05d}\")\n",
        "    else:\n",
        "        # Different months - need to find last valid ID in start month\n",
        "        print(f\"Multi-month mode: {start_prefix}.{start_id:05d} → {end_prefix}.{end_id:05d}\")\n",
        "\n",
        "        # Find last valid ID in start month\n",
        "        last_valid_start_month = find_last_valid_id(start_prefix, start_id, save_dir)\n",
        "\n",
        "        # Add papers from start month\n",
        "        for i in range(start_id, last_valid_start_month + 1):\n",
        "            paper_ids.append(f\"{start_prefix}.{i:05d}\")\n",
        "\n",
        "        # Add papers from end month (from 1 to end_id)\n",
        "        print(f\"\\nAdding papers from end month: {end_prefix}.00001 → {end_prefix}.{end_id:05d}\")\n",
        "        for i in range(1, end_id + 1):\n",
        "            paper_ids.append(f\"{end_prefix}.{i:05d}\")\n",
        "\n",
        "    return paper_ids\n",
        "\n",
        "\n",
        "def print_progress_report():\n",
        "    \"\"\"Print current statistics.\"\"\"\n",
        "    with stats_lock:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(\"CURRENT PROGRESS:\")\n",
        "        print(f\"  Total processed                       : {stats['total_processed']}\")\n",
        "        print(f\"  Both success                          : {stats['both_success']}\")\n",
        "        print(f\"  Only crawler success                  : {stats['only_crawler_success']}\")\n",
        "        print(f\"  Only references success               : {stats['only_references_success']}\")\n",
        "        print(f\"  Crawler failed                        : {stats['crawler_failed']}\")\n",
        "        print(f\"  References failed (404, Not Found)    : {stats['references_failed']}\")\n",
        "        print(f\"  Both failed                           : {stats['both_failed']}\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "\n",
        "def print_final_report():\n",
        "    \"\"\"Print final statistics with percentages.\"\"\"\n",
        "    total = stats['total_processed']\n",
        "\n",
        "    # Calculate success rates\n",
        "    both_success_rate = (stats['both_success'] / total * 100) if total > 0 else 0\n",
        "    phase2_fail_rate = (stats['references_failed'] / total * 100) if total > 0 else 0\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"FINAL REPORT:\")\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"CURRENT PROGRESS:\")\n",
        "    print(f\"  Total processed                       : {stats['total_processed']}\")\n",
        "    print(f\"  Both success                          : {stats['both_success']}\")\n",
        "    print(f\"  Only crawler success                  : {stats['only_crawler_success']}\")\n",
        "    print(f\"  Only references success               : {stats['only_references_success']}\")\n",
        "    print(f\"  Crawler failed                        : {stats['crawler_failed']}\")\n",
        "    print(f\"  References failed (404, Not Found)    : {stats['references_failed']}\")\n",
        "    print(f\"  Both failed                           : {stats['both_failed']}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    print(\"SUCCESS RATES:\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"  Both phases success rate : {both_success_rate:.2f}%\")\n",
        "    print(f\"  Phase 2 (references) fail: {phase2_fail_rate:.2f}%\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "\n",
        "def run_parallel_processing(start_month, start_id, end_month, end_id,\n",
        "                            max_parallels=5, save_dir=\"./23127238\"):\n",
        "    \"\"\"\n",
        "    Main function to run parallel processing of papers.\n",
        "\n",
        "    Args:\n",
        "        start_month: Start month in format \"YYYY-MM\"\n",
        "        start_id: Starting ID number\n",
        "        end_month: End month in format \"YYYY-MM\"\n",
        "        end_id: Ending ID number\n",
        "        max_parallels: Number of parallel threads (default: 5)\n",
        "        save_dir: Directory to save data\n",
        "    \"\"\"\n",
        "    # Reset stats\n",
        "    with stats_lock:\n",
        "        for key in stats:\n",
        "            stats[key] = 0\n",
        "\n",
        "    # Generate paper IDs\n",
        "    paper_ids = generate_paper_ids(start_month, start_id, end_month, end_id, save_dir)\n",
        "    total_papers = len(paper_ids)\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"STARTING PARALLEL PROCESSING\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Range: {start_month} ID {start_id} → {end_month} ID {end_id}\")\n",
        "    print(f\"Total papers to process: {total_papers}\")\n",
        "    print(f\"Parallel threads: {max_parallels}\")\n",
        "    print(f\"Output directory: {save_dir}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Process papers in parallel\n",
        "    with ThreadPoolExecutor(max_workers=max_parallels) as executor:\n",
        "        futures = {\n",
        "            executor.submit(process_paper, arxiv_id, save_dir): arxiv_id\n",
        "            for arxiv_id in paper_ids\n",
        "        }\n",
        "\n",
        "        completed = 0\n",
        "        for future in as_completed(futures):\n",
        "            arxiv_id = futures[future]\n",
        "            completed += 1\n",
        "\n",
        "            try:\n",
        "                paper_id, crawler_ok, refs_ok = future.result()\n",
        "                status = \"✓✓\" if (crawler_ok and refs_ok) else \\\n",
        "                         \"✓X\" if (crawler_ok and not refs_ok) else \\\n",
        "                         \"XX\"\n",
        "                print(f\"\\n[{completed}/{total_papers}] {status} {paper_id}\")\n",
        "\n",
        "                # Print progress every 10 papers\n",
        "                if completed % 10 == 0:\n",
        "                    print_progress_report()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\n[{completed}/{total_papers}] !! {arxiv_id} - Error: {e}\")\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    # Print final report with percentages\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"PROCESSING COMPLETE!\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Time elapsed: {elapsed_time:.2f} seconds\")\n",
        "    print(f\"Average time per paper: {elapsed_time/total_papers:.2f} seconds\" if total_papers > 0 else \"\")\n",
        "    print_final_report()\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"The main function to run the entire processing\"\"\"\n",
        "\n",
        "    # === CONFIGS ===\n",
        "    START_MONTH = \"2023-05\"\n",
        "    START_ID = 2001\n",
        "    END_MONTH = \"2023-05\"\n",
        "    END_ID = 4500\n",
        "    MAX_PARALLELS = 3\n",
        "    SAVE_DIR = \"./23127238\"\n",
        "\n",
        "    # STEP 1: MEASURE INITIAL RESOURCES (BASELINE)\n",
        "    print(\"=\"*50)\n",
        "    print(\"Baseline resource measurement...\")\n",
        "    disk_usage_start = shutil.disk_usage('/').used\n",
        "    ram_usage_start = psutil.virtual_memory().used\n",
        "    print(f\"  Disk ban đầu: {disk_usage_start / (1024**3):.2f} GB\")\n",
        "    print(f\"  RAM ban đầu : {ram_usage_start / (1024**3):.2f} GB\")\n",
        "\n",
        "    # STEP 2: START MONITORING THREAD\n",
        "    print(\"Start the resource monitor thread...\")\n",
        "    global monitor_running\n",
        "    monitor_running = True\n",
        "\n",
        "    monitor_thread = threading.Thread(\n",
        "        target=_monitor_resources,\n",
        "        args=(ram_usage_start, disk_usage_start, 2), # (baseline_ram, baseline_disk, 2s interval)\n",
        "        daemon=True\n",
        "    )\n",
        "    monitor_thread.start()\n",
        "\n",
        "    # STEP 3: RUN THE MAIN TASK\n",
        "    run_parallel_processing(\n",
        "        start_month=START_MONTH,\n",
        "        start_id=START_ID,\n",
        "        end_month=END_MONTH,\n",
        "        end_id=END_ID,\n",
        "        max_parallels=MAX_PARALLELS,\n",
        "        save_dir=SAVE_DIR\n",
        "    )\n",
        "\n",
        "    # STEP 4: STOP MONITORING & FINAL MEASUREMENT\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Task completed. Stopping monitoring stream...\")\n",
        "    monitor_running = False\n",
        "    monitor_thread.join() # Wait for the stream to completely shut down\n",
        "\n",
        "    disk_usage_end = shutil.disk_usage('/').used\n",
        "    print(f\"  Last disk: {disk_usage_end / (1024**3):.2f} GB\")\n",
        "\n",
        "    # STEP 5: PRINT ADDITIONAL REPORT\n",
        "    # (This function will print average RAM, peak Disk, last Disk)\n",
        "    _print_custom_resource_report(disk_usage_start, disk_usage_end)\n"
      ],
      "metadata": {
        "id": "AoKzTzp_P-V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext memory_profiler\n",
        "\n",
        "# Import your main.py file as a module\n",
        "import main\n",
        "import importlib\n",
        "\n",
        "importlib.reload(main)\n",
        "\n",
        "print(\"--- START MEASURING PEAK RAM ---\")\n",
        "%memit main.main()\n",
        "print(\"--- PEAK RAM MEASUREMENT END ---\")"
      ],
      "metadata": {
        "id": "qRsP5StkQBA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- COLAB DRIVE OVERVIEW ---\")\n",
        "!df -h /\n",
        "\n",
        "print(\"\\n--- REQUIRED OUTPUT CAPACITY ---\")\n",
        "# Measure output folder size\n",
        "!du -sh ./23127238"
      ],
      "metadata": {
        "id": "fpMiyG9EQBoR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}